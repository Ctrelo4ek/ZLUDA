<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Courtesy of https://github.com/LeoColomb/perfectmotherfuckingwebsite -->
  <style>
    body {
      max-width: 650px;
      margin: 40px auto;
      padding: 0 10px;
      font: 18px/1.5 -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
      color: #444;
    }

    h1,
    h2,
    h3 {
      line-height: 1.2;
    }

    h1 small, h2 small {
      font-size:16px;
      font-weight:normal;
    }

    @media (prefers-color-scheme: dark) {
      body {
        color: #c9d1d9;
        background: #0d1117;
      }

      a:link {
        color: #58a6ff;
      }

      a:visited {
        color: #8e96f0;
      }
    }
    
    
    /* Recommended code block styling (https://www.getzola.org/documentation/content/syntax-highlighting/#styling-codeblocks) */
    pre {
      padding: 1rem;
      overflow: auto;
    }
    /* The line numbers already provide some kind of left/right padding */
    pre[data-linenos] {
      padding: 1rem 0;
    }
    pre table td {
      padding: 0;
    }
    /* The line number cells */
    pre table td:nth-of-type(1) {
      text-align: center;
      vertical-align: top;
      user-select: none;
    }
    pre mark {
      /* If you want your highlights to take the full width */
      display: block;
      /* The default background colour of a mark is bright yellow */
      background-color: rgba(254, 252, 232, 0.9);
    }
    pre table {
      width: 100%;
      border-collapse: collapse;
    }
  </style>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ZLUDA - ZLUDA update Q1 2025 - roadmap update, LLVM tests, denormals</title>
</head>

<body>
  <section class="section">
    <div class="container">
    <h1>
      <div>
        <a style="color: #0d1117 !important; text-decoration:none;" href="https://vosen.github.io/ZLUDA">ZLUDA</a>
        <div style="float: right;">
          <a href="https://github.com/vosen/ZLUDA"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white"/></a> <a href="https://discord.gg/sg6BNzXuc7"><img src="https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white"/></a>
        </div>
      </div>
      <small>
        <p style="margin-top: 0.25em">ZLUDA allows to run unmodified CUDA applications on non-NVIDIA GPUs</p>
      </small>
    </h1>
      
<h2 class="title">
  ZLUDA update Q1 2025 - roadmap update, LLVM tests, denormals
  <small><div>2025-04-03</div></small>
</h2>
<p>Welcome to the new ZLUDA update. Read about our plans for the nearest future (that include PyTorch and PhysX) in <a href="https://vosen.github.io/ZLUDA/blog/zluda-update-q1-2025/#roadmap-update">Roadmap update</a> and about progress made this quarter in <a href="https://vosen.github.io/ZLUDA/blog/zluda-update-q1-2025/#llvm-bitcode-unit-tests">LLVM bitcode unit tests</a> and <a href="https://vosen.github.io/ZLUDA/blog/zluda-update-q1-2025/#correct-rounding-and-denormal-modes-on-amd-gpus">Correct rounding and denormal modes on AMD GPUs</a>.</p>
<h3 id="roadmap-update">Roadmap update</h3>
<h4 id="pytorch">PyTorch</h4>
<p>PyTorch remains my top priority and I still aim at being able to have PyTorch running on ZLUDA Q3/Q4 this year. Before PyTorch is up and running I am aiming for an intermediate goal: llm.c. You can see the progress towards getting llm.c up and running <a href="https://github.com/vosen/ZLUDA/milestone/5">here</a>.</p>
<h4 id="physx">PhysX</h4>
<p>As you might have read <a href="https://www.techpowerup.com/332763/nvidias-32-bit-physx-waves-goodbye-with-geforce-rtx-50-series-ending-32-bit-cuda-software-support">here</a>, <a href="https://www.tomshardware.com/pc-components/gpus/physx-quietly-retired-on-rtx-50-series-gpus-nvidia-ends-32-bit-cuda-app-support">here</a> and on multiple other sites, NVIDIA dropped support for 32-bit PhysX in their latest generation of GPUs, leaving a number of older games stranded.</p>
<p>This reignited the debate about <a href="https://github.com/vosen/ZLUDA/discussions/31">ZLUDA’s PhysX support</a>. After reading through it several times, it’s clear to me that there is a path in ZLUDA to rescuing those games and getting them to run on both AMD and NVIDIA GPUs.</p>
<p>I broke down the implementation into tasks <a href="https://github.com/vosen/ZLUDA/milestone/6">here</a>. If you can program Rust and want to make a lot of people happy, I encourage you to contribute. I won't be able to work on it myself because I'll be busy with PyTorch support, but I'll help in any way I can.</p>
<h3 id="llvm-bitcode-unit-tests">LLVM bitcode unit tests</h3>
<p>The ZLUDA compiler is the cornerstone of the project. It processes PTX modules by applying a series of transformations, ultimately generating LLVM bitcode. This LLVM bitcode is subsequently fed into the installed ROCm/HIP driver, which compiles it into a binary suitable for the currently installed GPU.</p>
<p>The compiler codebase includes multiple unit tests. Each test asserts that for:</p>
<ul>
<li>given PTX source code</li>
<li>given input data</li>
<li>given output data</li>
</ul>
<p>It can compile successfully and execute compiled binary with input data and produce the output data.</p>
<p>While this covers the entire end-to-end flow, there is a valuable sub-flow hiding here that could be tested too: the compilation from PTX to the LLVM bitcode. For each PTX source module, we could commit the compiled LLVM bitcode in a textual format and implement tests to ensure it remains unchanged. This approach is particularly useful for newly written complex compiler transformations that modify the emitted LLVM across the board. By using LLVM bitcode tests, you can observe how your modifications impact LLVM generation across various use cases, even those you might assume are unrelated.</p>
<p>This feature sat on the &quot;help wanted&quot; list for quite some time and I’m happy to see the first external contributor address this issue. JoelleJS merged it in <a href="https://github.com/vosen/ZLUDA/pull/324">#324</a>. Just in time for a significant feature that will use these tests.</p>
<h3 id="correct-rounding-and-denormal-modes-on-amd-gpus">Correct rounding and denormal modes on AMD GPUs</h3>
<p>This is an important feature that I have wanted to do for years. It is not present even in the old (pre-rollback) ZLUDA. The priority was always given to enabling new workloads, instead of making everything perfectly correct. Now we are out of proof-of-concept mode and can spend some time on correctness. As you will read below, it is a complex feature that is quite often invisible to the end user. It was acceptable for old ZLUDA do things incorrectly.</p>
<blockquote>
<p><strong>Warning</strong><br />
The remainder of this article assumes you know what PTX, floating-point numbers, control flow graphs, and basic blocks are. You don't need to be an expert, but a lack of familiarity with these concept will make everything below incomprehensible.</p>
</blockquote>
<p>If you know what floating-point denormals and rounding modes are you can skip to the next section (<a href="https://vosen.github.io/ZLUDA/blog/zluda-update-q1-2025/#previously-on-zluda">Previously on &quot;ZLUDA&quot; ...</a>).</p>
<p>First, some definitions. What exactly is denormal mode, and what are denormal numbers? Denormals (subnormals), represent a category of very small floating-point values. For the most common floating point size (32 bit), these values fall within the range of -3.4×1038 to 3.4×1038 (excluding 0). Due to the encoding of floating-point numbers, this category necessitates additional processing and has historically been either unsupported or supported with reduced performance. When we say &quot;unsupported,&quot; it means that denormal values are treated as zeros. In the context of PTX, denormal mode refers to a flag (<code>.ftz</code>) on floating-point instructions that determines whether they process denormal values or treat them as zeros, &quot;flushing to zero.&quot; In general, modern, mainstream hardware architectures can handle basic operations - add, multiply, fused multiply add, etc. - with denormal values at full speed.</p>
<p>Now rounding mode. Most of the &quot;simple&quot; operations floating-point operations are formally defined as &quot;performs the operation with infinite precision and then rounds infinite value to a finite value using chosen mode&quot;. Usual rounding modes are &quot;round to nearest even&quot;, &quot;round to zero&quot;, &quot;round to positive infinity&quot;, &quot;round to negative infinity&quot;. Rounding mode effectively controls the least-significant bit of the mantissa of the floating-point result. Although a single least-significant bit may seem insignificant, it can have a noticeable impact. For instance, consider two values that differ only by the least significant bit: 1.0000000 and 1.0000001. In certain contexts, the difference of 0.0000001 can be substantial.</p>
<p>Now that we understand the denormal and rounding part, let's focus on the mode part. Typically, CPUs will do some mix of integer calculations and floating-point calculations, with the specific proportions varying based on the workload. In contrast, GPUs—regardless of whether they are tailored for gaming, high-performance computing (HPC), or machine learning—primarily dedicate their processing cycles to floating-point operations. This focus prompts GPU architects to prioritise floating-point support in their hardware designs.</p>
<p>One notable feature found in NVIDIA hardware, and consequently in PTX, is the per-instruction control for denormal and rounding operations. In a CPU, a common approach to managing this issue is to implement a global control (as seen in x86 and ARM architectures) or to forgo denormal control altogether (as in RISC-V). While this design choice is beneficial for programmers, it presents unique challenges for ZLUDA when translating to an AMD GPU which uses global control (like a CPU).</p>
<h4 id="previously-on-zluda">Previously on &quot;ZLUDA&quot; ...</h4>
<p>Pre-rollback ZLUDA used the simplest possible approach that almost works:</p>
<ul>
<li>For denormal mode (which is either &quot;flush-to-zero&quot; or &quot;preserve denormals&quot;) hold a &quot;vote&quot; for each function. Count the number of instructions using each mode and then just use the more prolific mode across the function</li>
<li>For rounding mode, ignore it completely and always use &quot;round to nearest even&quot;</li>
</ul>
<p>PTX module compiled from C++ CUDA sources will usually use the same denormal mode across the whole module with particular mode depending on the compiler flags. Rounding mode use is somewhat uncommon.</p>
<p>Sure, this approach is not correct, but it worked somewhat okayish and it led to only a single major bug (that I’ve noticed). Still, ZLUDA is now out of proof-of-concept mode and we are now doing things correctly.</p>
<h4 id="dead-end-1-llvm-hip-rocm">Dead end#1: LLVM &amp; HIP/ROCm</h4>
<p>When implementing a new compiler feature in ZLUDA, the first step is to check if it's implemented by the baseline LLVM. The perfect LLVM support would allow ZLUDA to do a trivial per-instruction transformation like this:</p>
<p>from (PTX pseudocode):</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>z = add.ftz x, y
</span><span>a = add.ftz b, c
</span></code></pre>
<p>to (LLVM pseduocode):</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>old_fpstate1 = llvm.get_fpstate()
</span><span>llvm.set_ftz(true)
</span><span>z = add x, y
</span><span>llvm.set_fpstate(old_fpstate1)
</span><span>old_fpstate2 = llvm.get_fpstate()
</span><span>llvm.set_ftz(true)
</span><span>a = add b, c
</span><span>llvm.set_fpstate(old_fpstate2)
</span></code></pre>
<p>and have LLVM optimize that to (AMD GPU assembler pseudocode):</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>S_DENORM_MODE flush, flush
</span><span>V_ADD_NC_U32 z, x, y
</span><span>V_ADD_NC_U32 a, b, c
</span></code></pre>
<p>The initial research on LLVM floating point builtins appeared promising, as this collection of intrinsics seemed to address our specific use case:</p>
<ul>
<li><code>llvm.get.fpenv/llvm.set.fpenv</code></li>
<li><code>llvm.get.fpmode/llvm.set.fpmode</code></li>
<li><code>llvm.experimental.*</code> family</li>
</ul>
<p>Sadly, they are all deficient in some way. They either compile down to poor, unoptimized AMD GPU code or do not work at all . Granted, <code>llvm.experimental.*</code> support is being worked on by AMD and should appear in the future ROCm versions, but this does not help us today.</p>
<p>This raises the question: in CUDA C++ you have a bunch of builtins to do operations with the specified rounding mode, e.g. <code>__fadd_rz</code> for floating point addition with &quot;round-to-zero&quot; mode. What happens on ROCm?</p>
<p>Further exploration revealed (<a href="https://rocm.docs.amd.com/projects/HIP/en/latest/reference/math_api.html#floating-point-intrinsics">source</a>):</p>
<blockquote>
<p>Only the nearest-even rounding mode is supported by default on AMD GPUs. The <code>_rz</code>, <code>_ru</code>, and <code>_rd</code> suffixed intrinsic functions exist in the HIP AMD backend if the <code>OCML_BASIC_ROUNDED_OPERATIONS</code> macro is defined.</p>
</blockquote>
<p>Ok. You can use those functions, but they are hidden behind a define. That’s weird. Time to try it!</p>
<p>The HIP/ROCm source code:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>#include &lt;hip/hip_runtime.h&gt;
</span><span>
</span><span>__global__ void foobar(int* array, int n) {
</span><span>    int tid = blockDim.x * blockIdx.x + threadIdx.x;
</span><span>    array[tid] = __fadd_rz(array[tid], array[tid]);
</span><span>    array[tid+1] = __fadd_rz(array[tid+1], array[tid+1]);
</span><span>}
</span></code></pre>
<p>When using ROCm 6.3, compiles down to this (some output omitted for clarity):</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>0000000000001900 &lt;__ocml_add_rtz_f32&gt;:
</span><span>        s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
</span><span>        s_setreg_imm32_b32 hwreg(HW_REG_MODE, 0, 2), 3
</span><span>        v_add_f32_e32 v0, v0, v1
</span><span>        s_setreg_imm32_b32 hwreg(HW_REG_MODE, 0, 2), 0
</span><span>        s_setpc_b64 s[30:31]
</span><span>
</span><span>...
</span><span>
</span><span>0000000000001a00 &lt;_Z6foobarPii&gt;:
</span><span>        ...
</span><span>        s_getpc_b64 s[0:1]
</span><span>        s_add_u32 s0, s0, 0xfffffeac
</span><span>        s_addc_u32 s1, s1, -1
</span><span>        ...
</span><span>        s_swappc_b64 s[30:31], s[0:1]
</span><span>        v_cvt_f32_i32_e32 v1, v5
</span><span>        v_cvt_i32_f32_e32 v4, v0
</span><span>        v_mov_b32_e32 v0, v1
</span><span>        s_swappc_b64 s[30:31], s[0:1]
</span><span>        s_delay_alu instid0(VALU_DEP_1)
</span><span>        v_cvt_i32_f32_e32 v5, v0
</span><span>        global_store_b64 v[2:3], v[4:5], off
</span><span>        s_endpgm
</span></code></pre>
<p>Ok, mystery solved. I too, would like to hide this compiler output.</p>
<p>For those of us who are not proficient in AMD GPU assembly: every use of <code>__fadd_rz</code> requires a function call (<code>s_swappc_b64</code>, expensive) and two calls to set rounding mode (<code>s_setreg_imm32_b32</code>, also expensive). This is simply too much overhead to be acceptable. </p>
<p>We are going to build our own support. Our goal, for the code above, is a single instruction to set the rounding mode (or even zero instructions as we will see later).</p>
<h4 id="building-support-in-zluda">Building support in ZLUDA</h4>
<p>Our new goal is to write a complete transformation (compiler pass) in ZLUDA that will insert instructions that set the global modes (rounding and denormal). We want to insert as few instructions as possible for the best possible performance - there’s no LLVM pass that is going to optimize the insertions for us.</p>
<p>Let’s take half of a step back. We know that the trivial (and slow) approach is to simply set the global mode before every instruction that makes use of a mode. It can be improved by omitting the mode-setting instructions if we know that the previous instruction uses the same mode. We can always track this in straight-line code, but what happens if there are branches? What happens if there are multiple branches with from different sources, but into the same target? It seems to be sufficient to figure out which branches require mode change and which do not.</p>
<p>This leads us to a new reformulation. We can express this problem as a control flow graph augmented with a little bit of extra information: for each mode (denormal, rounding) each node (basic block) will have &quot;entry&quot; state and &quot;exit&quot; state. Entry state for a basic block is the mode of the first mode-using instruction in the basic block. Similarly, exit mode is the mode of the last mode-using instruction. This simplifies problem quite a bit. We must now compute which edges (jumps) in the control flow graph require an insertion of mode change.</p>
<p>For illustrative purpose we will only consider mode that takes two values: true (green) and false (red). Picture below is node &quot;A&quot; that has a &quot;true&quot; entry mode and &quot;false&quot; exit mode and jumps to node B that has &quot;true&quot; entry mode and &quot;false&quot; exit mode:</p>
<p align="center">
<img src="25q1-1.svg" alt="drawing" width="25%"/>
</p>
<h4 id="dead-end-2-mode-forward-propagation">Dead end #2: mode forward propagation</h4>
<p>Something I did not mention explicitly, but is important: some nodes lack both entry and exit modes. Consider the following example:</p>
<p align="center">
<img src="25q1-2.svg" alt="drawing" width="25%"/>
</p>
<p>There’s no need to enter mode-setting instruction - node B will propagate the &quot;false&quot; value, but in this example:</p>
<p align="center">
<img src="25q1-3.svg" alt="drawing" width="25%"/>
</p>
<p>we need to insert mode change from &quot;false&quot; to &quot;true&quot; somewhere between nodes A and C.</p>
<p>My first instinct was to propagate modes forward: for each node propagate its exit mode to all its successor nodes. While it is instinctively correct and solves two examples above, there are two problems:</p>
<ul>
<li>
<p>It’s relatively awakward to implement. Remember, a node can have more than one predecessor nodes. What happens if there is a node with an empty incoming edge and a &quot;true&quot; incoming edge? Should we do post-processing?</p>
</li>
<li>
<p>More concretely, this does not really handle codependence patterns like this:</p>
<p align="center"><img src="25q1-4.svg" width="80%"/></p>
In this example node A can’t propagate its mode to B or C outright because they have more incoming edges. B and C can’t propagate their mode either because they have no mode - they depend on A.
</li>
</ul>
<h4 id="better-approach-backward-propagation">Better approach: backward propagation</h4>
<p>Dependency problems from the previous solution hint at a better approach: backward propagation. Instead of propagating the exit mode we can compute the set of incoming modes. This set is the set of all possible values a given mode can have on the first instruction of the basic block. Sounds complex, but can be computed easily if you have our augmented control flow graph. Take all incoming nodes and if an incoming node’s exit mode is non-empty then add that value to the set, if the incoming node’s exit mode is empty then recursively check its incoming nodes.</p>
<p>We now have the core of our algorithm, but it’s not a complete solution yet: the realities of AMD GPU hardware make it far more complex.</p>
<h4 id="hardware-quirks">Hardware quirks</h4>
<p>When targeting AMD GPUs, there are several hardware properties that we should take into account:</p>
<ul>
<li>
<p>Kernel, on startup, has a certain initial state that is controlled by the programmer (or the compiler in our case). Part of the initial state is the initial state of denormal and rounding registers (global modes). We get this initial mode for free, no extra instructions needed</p>
</li>
<li>
<p>Each mode (denormal and rounding) is actually split into two registers (global modes). One for f32 and one for joint f16 and f64. In total there are four registers: denormal f32, denormal f16+f64, rounding f32, rounding f16+f64</p>
</li>
</ul>
<p>Registers (global modes) of the same kind (denormal, rounding), but with different width (f32, f16+f64) are for our purpose twin registers. One quirk of AMD GPU is that there are three instructions for settings global mode: <code>S_SETREG</code> to set any hardware (non-generic purpose) register and <code>S_ROUND_MODE</code>, <code>S_DENORM_MODE</code> to set just the rounding or denormal mode. <code>S_ROUND_MODE</code>, <code>S_DENORM_MODE</code> are much cheaper than <code>S_SETREG</code>. The annoying limitation of <code>S_ROUND_MODE</code>, <code>S_DENORM_MODE</code> is that they can only set both f32 and f16+f64. For this reason we will only do mode insertions for both f32 and f16+f64</p>
<h4 id="final-algorithm">Final algorithm</h4>
<p>If you made it this far, congratulations, you made it through the introduction. Now we can start implementing our algorithm.</p>
<h5 id="create-control-flow-graph">Create control flow graph</h5>
<p>Our first step is to compute the control flow graph. Every basic block contains entry and exit mode. For efficiency each node actually contains four entry modes and four exit modes. One for each AMD GPU mode: denormal f32, denormal f16+f64, rounding f32, rounding f16+f64.</p>
<p>We handle function calls by including them in the graph. Call from function &quot;foo&quot; to function &quot;bar&quot; is expressed as a node from the caling basic block of &quot;foo&quot; to the first basic block of &quot;bar&quot;. We don’t support virtual calls in the current ZLUDA, because they are extremally rare. They can be easily added later.</p>
<p>During this step we compute both entry and exit mode for each basic block. Additionally, each kernel starts with an artificial starting node. This node get a special &quot;entry&quot; and &quot;exit&quot; value: the numeric identifier of the kernel. This numeric identifier is used across the whole ZLUDA compiler. It is already present (generated by previous compiler passes) and unique for a kernel. For example: while denormals register can take one of two values: true or false, in our CFG, the values that represent denormals can be true, false or arbitrary numeric id of a kernel. While going from a bounded to an ubounded set does not intuitively sound like a good decision, it’s temporary. We will optimize it back to the bounded set soon.</p>
<h5 id="compute-minimal-insertions">Compute minimal insertions</h5>
<p>Our next goal is, for each of the four modes, compute minimal set of insertions. In other words: figure out which basic blocks can be reached with different mode than expected by the first instruction. We do this computation for each of the four modes separately.</p>
<p>We start by computing two sets: required insertions and potential insertions. We choose nodes which have an entry mode (we skip the nodes with empty entry mode and kernel nodes with numeric ids). Then, for each node, we compute the set of incoming modes:</p>
<ul>
<li>If the set contains a value that is different from the node’s entry mode then we add the node to required insertions</li>
<li>If the set of incoming modes is purely a set of kernel numeric ids (with no conflicting specific mode values) then we add the node id along with its mode and kernel ids to the potential insertions</li>
</ul>
<p>Required insertions are set in stone: if we jump from another node with different mode then we must insert a mode set instruction. Potential insertions on the other hand can be omitted: for a given node, if all the related kernels have the same initial value as the node then we can skip the mode set instruction.</p>
<p>E.g. if we have kernels &quot;foo&quot; and &quot;bar&quot; that both call function &quot;asdf&quot; and &quot;asdf&quot; entry mode is &quot;true&quot;, then we should set initial mode for &quot;foo&quot; and &quot;bar&quot; to &quot;true&quot; and avoid inserting additional mode-setting instructions.</p>
<p>The problem is easy to solve in the example above, the general case is not trivial. I could not come up with a non-brute force algorithm and opted to encode the problem as an integer linear programming problem and use an external solver. <a href="https://cs.stackexchange.com/questions/12102/express-boolean-logic-operations-in-zero-one-integer-linear-programming-ilp/12118#12118">This excellent post</a> helped encode my constraints. As for the solver I went with <a href="https://github.com/Specy/microlp">microlp</a>, mainly because it’s a relatively small dependency. I wanted to avoid dragging something big like SCIP or even Z3 into the project. Our problem sizes are not going to be big. PTX modules tend to have a handful of kernels and simple control flow.</p>
<h5 id="compute-full-insertions">Compute full insertions</h5>
<p>Now we have:</p>
<ul>
<li>Provisional control flow graph (with some nodes empty and kernel starting nodes containing numeric ids instead of specific values)</li>
<li>List of nodes that require a mode change on entry (if the incoming mode is different - there might be multiple nodes incoming, each with their own mode)</li>
<li>For kernels that were subject to optimization in the previous step: its initial state</li>
</ul>
<p>We are almost ready to start inserting <code>S_ROUND_MODE</code> and <code>S_DENORM_MODE</code>. We have all the necessary information, we just need to do some more preprocessing. Specifically we need to know two things:</p>
<ul>
<li>
<p>What is the effective entry mode for each block<br />
Note that even though mode instructions are inserted along edges in the CFG (jumps in code), we don’t explictly store edges. That’s because when inserting mode-setting instructions in a basic blocks we will implictly calculate exit mode anyway. And since we know what identifier we jump into, as long as we have information what are the modes of our jump target we know if they are different and in consequence if the jump requires a mode change</p>
</li>
<li>
<p>What is the exit mode for a function<br />
This is necessary because functions calls are mechanically different from normal jumps. Function calls terminate a basic block and we need to know if the new basic block starting from the first post-call instruction requires a mode change. Since a function can be called from many places it is a responsibility of the caller to do post-call mode adjustments (if necessary)</p>
</li>
</ul>
<p>Computing both of those is relatively straightforward. First, we take our incomplete control flow graph and resolve all empty nodes and special kernel nodes. For empty nodes we compute the incoming set - if the set contains more than a single value, we use a special value &quot;conflict&quot;. For special starting kernel nodes we have a list of kernel with their initial values from the previous optimization pass.</p>
<p>Lastly, we join four separate logical CFGs (each for one AMD GPU mode) into two lookup tables. One lookup table contains all the necessary information to support mode changes for branches, the other lookup table contains all the necessary information to support mode changes for functions calls.</p>
<h5 id="apply-mode-control">Apply mode control</h5>
<p>In this stage we walk through every function (kernel and non-kernel) and modify it accordingly:</p>
<ul>
<li>If necessary, insert mode change &quot;prelude&quot; basic block before each basic block</li>
<li>If necessary, redirect branch to go into mode change &quot;prelude&quot;</li>
<li>Insert all mode changes inside a basic block. We fold twin registers together. For example pseudocode like this:<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>add.ftz.f32 a, b, c;
</span><span>add.no_ftz.f16 x, y, z;
</span></code></pre>
gets converted into this pseudocode:<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>set_denormal.f32.f16 ftz, no_ftz;
</span><span>add.f32 a, b, c;
</span><span>add.f16 x, y, z;
</span></code></pre>
</li>
</ul>
<p>After all this hard work we now get a new module with a small number of freshly inserted mode change instructions. It’s not optimal in the absolute sense, but it’s much better than the alternatives. The AMD GPU code is now as correct as we can make it. Unfortunately, after all this hard work, our code can still miscompute some code. Read below for more.</p>
<h4 id="llvm-sadness">LLVM sadness</h4>
<p>Sadly, there are still some issues outside of our control.</p>
<p>Firstly, a minor issue. As mentioned previously, for each AMD GPU kernel we can sat initial denormal mode and initial rounding mode. This is true in the general sense, but for some reason LLVM AMDGPU backend exposes the control for initial denormal mode, but not for initial rounding mode. Right now, we set initial rounding mode by inserting the instruction for it at the start of the kernel. We could skip this single instruction with better LLVM AMD GPU support.</p>
<p>Secondly, a bigger issue. Hardware-agnostic LLVM passes don’t understand AMD GPU instructions that set global state. So this pseudocode:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>set_denormal.f32.f16 ftz, ftz;
</span><span>add.f32 x, b, c;
</span><span>set_denormal.f32.f16 no_ftz, no_ftz;
</span><span>add.f32 y, b, c;
</span></code></pre>
<p>after LLVM optimizations ends up as:</p>
<pre style="background-color:#2b303b;color:#c0c5ce;"><code><span>set_denormal.f32.f16 ftz, ftz;
</span><span>add.f32 x, b, c;
</span><span>mov.f32 y, x;
</span></code></pre>
<p>Which gives incorrect result. While it’s rare to see the same input being computed twice with different modes, it’s concerning.</p>
<p>Fixing this would require deeper changes in LLVM (making mode part of the instruction, like in llvm.experimental.constrained.*) and probably porting this pass to LLVM. We might do eventually do it, but that’s enough effort for now.</p>
<hr />
<p>If you made it this far, let me know in the comments what do you think. See you next time.</p>


<script src="https://giscus.app/client.js"
        data-repo="vosen/zluda_website"
        data-repo-id="R_kgDOM5Co2g"
        data-category="Announcements"
        data-category-id="DIC_kwDOM5Co2s4Ci6Tj"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

    </div>
  </section>
</body>

</html>