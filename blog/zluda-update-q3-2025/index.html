<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Courtesy of https://github.com/LeoColomb/perfectmotherfuckingwebsite -->
  <style>
    body {
      max-width: 650px;
      margin: 40px auto;
      padding: 0 10px;
      font: 18px/1.5 -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
      color: #444;
    }

    h1,
    h2,
    h3 {
      line-height: 1.2;
    }

    h1 small, h2 small {
      font-size:16px;
      font-weight:normal;
    }

    @media (prefers-color-scheme: dark) {
      body {
        color: #c9d1d9;
        background: #0d1117;
      }

      a:link {
        color: #58a6ff;
      }

      a:visited {
        color: #8e96f0;
      }
    }
    
    
    /* Recommended code block styling (https://www.getzola.org/documentation/content/syntax-highlighting/#styling-codeblocks) */
    pre {
      padding: 1rem;
      overflow: auto;
    }
    /* The line numbers already provide some kind of left/right padding */
    pre[data-linenos] {
      padding: 1rem 0;
    }
    pre table td {
      padding: 0;
    }
    /* The line number cells */
    pre table td:nth-of-type(1) {
      text-align: center;
      vertical-align: top;
      user-select: none;
    }
    pre mark {
      /* If you want your highlights to take the full width */
      display: block;
      /* The default background colour of a mark is bright yellow */
      background-color: rgba(254, 252, 232, 0.9);
    }
    pre table {
      width: 100%;
      border-collapse: collapse;
    }
  </style>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ZLUDA - ZLUDA update Q3 2025 – ZLUDA 5 is here</title>
</head>

<body>
  <section class="section">
    <div class="container">
    <h1>
      <div>
        <a style="color: #0d1117 !important; text-decoration:none;" href="https://vosen.github.io/ZLUDA">ZLUDA</a>
        <div style="float: right;">
          <a href="https://github.com/vosen/ZLUDA"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white"/></a> <a href="https://discord.gg/sg6BNzXuc7"><img src="https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white"/></a>
        </div>
      </div>
      <small>
        <p style="margin-top: 0.25em">ZLUDA allows to run unmodified CUDA applications on non-NVIDIA GPUs</p>
      </small>
    </h1>
      
<h2 class="title">
  ZLUDA update Q3 2025 – ZLUDA 5 is here
  <small><div>2025-10-02</div></small>
</h2>
<p>We're happy to announce the release of ZLUDA version 5. This release brings with it new debugging tools, better correctness and preliminary support for llama.cpp.</p>
<p>ZLUDA version 5 includes a tool called <code>zluda_trace</code>. Community members often ask us what they can do to help with the project. One of the most impactful things you can do without needing programming skills is to run your favorite workload using <code>zluda_trace</code> and create a <a href="https://github.com/vosen/ZLUDA/issues/new?template=zluda_dump.yml">bug report issue</a> with the trace attached. Just make sure you collect logs on Linux, we are not yet ready accept logs from Windows (more <a href="https://github.com/vosen/ZLUDA/issues/522">here</a>). You can find more information on our <a href="https://zluda.readthedocs.io/latest/troubleshooting.html">Troubleshooting</a> page.</p>
<p>And if you are interested in writing code, we have a list of issues labeled <a href="https://github.com/vosen/ZLUDA/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22help%20wanted%22">help wanted</a> on our repository.</p>
<h1 id="zoc-zluda-offline-compiler">zoc (ZLUDA offline compiler)</h1>
<p>ZLUDA includes an NVIDIA PTX to AMD RDNA compiler. Previously, this compiler was only accessible from the ZLUDA library – it runs when <code>cuModuleLoadData</code> or <code>cuLibraryLoadData</code> are called. However, for ZLUDA developers, it is useful for debugging purposes to have a command line interface as well, similar to NVIDIA's <code>ptxas</code> tool.</p>
<p>For this purpose, <a href="https://github.com/JoelleJS">JoelleJS</a> has contributed <code>zoc</code> in <a href="https://github.com/vosen/ZLUDA/pull/344">#344</a>, the ZLUDA offline compiler. This compiler takes a PTX file as input, and will output the LLVM IR generated by ZLUDA before and after linking and the RDNA assembly for your GPU generated by the ROCm compiler.</p>
<p>We've been using this tool intensively and merged minor ergonomics improvements in <a href="https://github.com/vosen/ZLUDA/pull/491">#491</a> and <a href="https://github.com/vosen/ZLUDA/pull/504">#504</a>.</p>
<h1 id="machine-learning-workloads">Machine Learning Workloads</h1>
<p>Our focus on running machine learning inference workloads continues. In this release, we have prioritized correctness over performance, which will be an area of focus for future updates.</p>
<h2 id="llm-c">llm.c</h2>
<p>We hit our first ML milestone.</p>
<p>llm.c's <code>test_gpt2fp32cu</code> and <code>test_gptc2cu</code> now both run on ZLUDA, when built without Multi-GPU and without Flash Attention. Support for Flash Attention is right now blocked by missing APIs in MIOpen. We plan to backfill them in the future.</p>
<p>This took a large number of commits across our host API implementation and compiler, including <a href="https://github.com/vosen/ZLUDA/pull/402">#402</a>, <a href="https://github.com/vosen/ZLUDA/pull/406">#406</a>, <a href="https://github.com/vosen/ZLUDA/pull/412">#412</a>, <a href="https://github.com/vosen/ZLUDA/pull/417">#417</a>, <a href="https://github.com/vosen/ZLUDA/pull/409">#409</a>, <a href="https://github.com/vosen/ZLUDA/pull/421">#421</a>, <a href="https://github.com/vosen/ZLUDA/pull/427">#427</a>, <a href="https://github.com/vosen/ZLUDA/pull/454">#454</a>, <a href="https://github.com/vosen/ZLUDA/pull/463">#463</a>, <a href="https://github.com/vosen/ZLUDA/pull/468">#468</a>, <a href="https://github.com/vosen/ZLUDA/pull/496">#496</a>, <a href="https://github.com/vosen/ZLUDA/pull/500">#500</a>, <a href="https://github.com/vosen/ZLUDA/pull/501">#501</a>, <a href="https://github.com/vosen/ZLUDA/pull/503">#503</a>, and <a href="https://github.com/vosen/ZLUDA/pull/511">#511</a>, in addition to the performance library work mentioned below.</p>
<h2 id="llama-cpp">llama.cpp</h2>
<p>We hit our second ML milestone.</p>
<p>The CUDA backend for llama.cpp can now run on ZLUDA. We've done some preliminary measurements and found the performance to be within range of the results measured by Phoronix on ROCm (<a href="https://www.phoronix.com/review/llama-cpp-windows-linux/5">Latest Open-Source AMD Improvements Allowing For Better Llama.cpp AI Performance Against Windows 11 - Phoronix</a>). We're interested in your feedback, if it doesn't work or you are getting worse performance than with ROCm, <a href="https://github.com/vosen/ZLUDA/issues">please share in the issues</a>.</p>
<p>Much of the required functionality in the host API and compiler was implemented at this point, so this took a relatively fewer number of commits to enable, including <a href="https://github.com/vosen/ZLUDA/pull/509">#509</a>, <a href="https://github.com/vosen/ZLUDA/pull/515">#515</a>, and <a href="https://github.com/vosen/ZLUDA/pull/518">#518</a>.</p>
<h2 id="initial-pytorch-work">Initial PyTorch work</h2>
<p>We did not yet hit our third ML milestone</p>
<p>We've been continuing to work on PyTorch support, which is our next big milestone. Other than work on host functions and instruction support in the compiler, we have added the <code>zluda_ld</code> library (<a href="https://github.com/vosen/ZLUDA/pull/447">#447</a> and <a href="https://github.com/vosen/ZLUDA/pull/508">#508</a>). PyTorch uses the <code>DT_RPATH</code> attribute in its executables to hard-code the path to the CUDA library and so ignores the <code>LD_LIBRARY_PATH</code> attribute we normally use to nudge applications to load ZLUDA on Unix. <code>zluda_ld</code> can be used with the little-known <code>LD_AUDIT</code> environment variable to get around this problem, and force loading ZLUDA.</p>
<p>PyTorch is far from being ready: we are blocked by the slowness of our compiler, missing performance libraries (cuBLAS, cuDNN, etc) coverage and bugs (or missing features) in LLVM AMDGPU target. This quarter we will be focusing on all those problems – please check our prerelease builds from time to time.</p>
<h1 id="other-improvements">Other improvements</h1>
<h2 id="kernel-cache">Kernel cache</h2>
<p>We have added a kernel caching mechanism to our PTX module loader (<a href="https://github.com/vosen/ZLUDA/pull/465">#465</a>). When a CUDA application loads a GPU code module, we need to extract the PTX from the fat binary provided and then compile it to machine code for the specific GPU being used. This can be a costly operation and significantly slow down runtime for a workload with many modules. We now avoid this by locally caching the machine code for kernels.</p>
<h2 id="performance-libraries">Performance libraries</h2>
<p>We have added initial support for running applications that use cuBLAS, cuBLASLt, and nvml (<a href="https://github.com/vosen/ZLUDA/pull/440">#440</a>, <a href="https://github.com/vosen/ZLUDA/pull/444">#444</a>, <a href="https://github.com/vosen/ZLUDA/pull/449">#449</a>, <a href="https://github.com/vosen/ZLUDA/pull/452">#452</a>, <a href="https://github.com/vosen/ZLUDA/pull/455">#455</a>, <a href="https://github.com/vosen/ZLUDA/pull/457">#457</a>, <a href="https://github.com/vosen/ZLUDA/pull/481">#481</a>). The number of supported operations is still small, but it's set up for rapid additions. Expect to see the list of supported functions grow (and the addition of cuDNN).</p>
<h2 id="more-testing">More testing</h2>
<p>We've set up CI, including running unit tests for every PR (<a href="https://github.com/vosen/ZLUDA/pull/401">#401</a>) and running our PTX sweep test suite nightly. This will help us prevent regressions and measure our conformance to CUDA behavior. We also have set up some initial scaffolding for testing the host API.</p>
<h2 id="prerelease-builds">Prerelease builds</h2>
<p>We've started publishing preview (prerelease) builds. Now, after every code change we automatically compile and publish the binary into the <a href="https://github.com/vosen/ZLUDA/releases">Release section on Github</a>. Try them out. There's no longer any reason to do build from sources by yourself (unless you are a developer).</p>
<h2 id="final-bit-of-correctness">Final bit of correctness</h2>
<p>CI improvements unlocked a flurry of compiler fixes in <a href="https://github.com/vosen/ZLUDA/pull/416">#416</a>, <a href="https://github.com/vosen/ZLUDA/pull/467">#467</a> and more. According to our testing we are now bit-accurate (we return results within CUDA-documented precision) with NVIDIA GPUs across almost all supported operations and their variants with all floating-point subnormal and rounding control modes. There are two exceptions:</p>
<ul>
<li>32-bit floating-point square root with non-default rounding modes (which is very rare)</li>
<li>64-bit floating-point transcendentals (division, square root, etc.)</li>
</ul>
<p>Some widely used instructions are still not supported by the compiler, but that number gets smaller every day.</p>
<p>Overall, this is a major improvement over pre-rollback ZLUDA, which would cut corners and not always be bit-accurate.</p>


<script src="https://giscus.app/client.js"
        data-repo="vosen/zluda_website"
        data-repo-id="R_kgDOM5Co2g"
        data-category="Announcements"
        data-category-id="DIC_kwDOM5Co2s4Ci6Tj"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

    </div>
  </section>
</body>

</html>